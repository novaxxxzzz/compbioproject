{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a2fc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 1.002 | Acc: 0.546\n",
      "Epoch 02 | Loss: 0.969 | Acc: 0.547\n",
      "Epoch 03 | Loss: 0.906 | Acc: 0.604\n",
      "Epoch 04 | Loss: 0.852 | Acc: 0.621\n",
      "Epoch 05 | Loss: 0.827 | Acc: 0.621\n",
      "Epoch 06 | Loss: 0.821 | Acc: 0.626\n",
      "Epoch 07 | Loss: 0.817 | Acc: 0.623\n",
      "Epoch 08 | Loss: 0.816 | Acc: 0.621\n",
      "Epoch 09 | Loss: 0.815 | Acc: 0.626\n",
      "Epoch 10 | Loss: 0.815 | Acc: 0.624\n",
      "Epoch 11 | Loss: 0.813 | Acc: 0.620\n",
      "Epoch 12 | Loss: 0.814 | Acc: 0.620\n",
      "Epoch 13 | Loss: 0.814 | Acc: 0.620\n",
      "Epoch 14 | Loss: 0.811 | Acc: 0.615\n",
      "Epoch 15 | Loss: 0.812 | Acc: 0.626\n",
      "Epoch 16 | Loss: 0.811 | Acc: 0.628\n",
      "Epoch 17 | Loss: 0.811 | Acc: 0.625\n",
      "Epoch 18 | Loss: 0.811 | Acc: 0.623\n",
      "Epoch 19 | Loss: 0.811 | Acc: 0.630\n",
      "Epoch 20 | Loss: 0.809 | Acc: 0.621\n",
      "Epoch 21 | Loss: 0.811 | Acc: 0.624\n",
      "Epoch 22 | Loss: 0.809 | Acc: 0.619\n",
      "Epoch 23 | Loss: 0.810 | Acc: 0.628\n",
      "Epoch 24 | Loss: 0.809 | Acc: 0.625\n",
      "Epoch 25 | Loss: 0.808 | Acc: 0.624\n",
      "Epoch 26 | Loss: 0.808 | Acc: 0.614\n",
      "Epoch 27 | Loss: 0.808 | Acc: 0.623\n",
      "Epoch 28 | Loss: 0.808 | Acc: 0.624\n",
      "Epoch 29 | Loss: 0.807 | Acc: 0.611\n",
      "Epoch 30 | Loss: 0.806 | Acc: 0.618\n",
      "Epoch 31 | Loss: 0.807 | Acc: 0.624\n",
      "Epoch 32 | Loss: 0.805 | Acc: 0.626\n",
      "Epoch 33 | Loss: 0.806 | Acc: 0.616\n",
      "Epoch 34 | Loss: 0.805 | Acc: 0.624\n",
      "Epoch 35 | Loss: 0.804 | Acc: 0.624\n",
      "Epoch 36 | Loss: 0.805 | Acc: 0.626\n",
      "Epoch 37 | Loss: 0.804 | Acc: 0.611\n",
      "Epoch 38 | Loss: 0.804 | Acc: 0.613\n",
      "Epoch 39 | Loss: 0.803 | Acc: 0.622\n",
      "Epoch 40 | Loss: 0.801 | Acc: 0.621\n",
      "Epoch 41 | Loss: 0.802 | Acc: 0.622\n",
      "Epoch 42 | Loss: 0.802 | Acc: 0.622\n",
      "Epoch 43 | Loss: 0.801 | Acc: 0.624\n",
      "Epoch 44 | Loss: 0.801 | Acc: 0.620\n",
      "Epoch 45 | Loss: 0.800 | Acc: 0.620\n",
      "Epoch 46 | Loss: 0.800 | Acc: 0.620\n",
      "Epoch 47 | Loss: 0.800 | Acc: 0.621\n",
      "Epoch 48 | Loss: 0.799 | Acc: 0.612\n",
      "Epoch 49 | Loss: 0.798 | Acc: 0.619\n",
      "Epoch 50 | Loss: 0.798 | Acc: 0.616\n",
      "Epoch 01 | Loss: 1.000 | Acc: 0.546\n",
      "Epoch 02 | Loss: 0.974 | Acc: 0.547\n",
      "Epoch 03 | Loss: 0.912 | Acc: 0.601\n",
      "Epoch 04 | Loss: 0.855 | Acc: 0.600\n",
      "Epoch 05 | Loss: 0.831 | Acc: 0.613\n",
      "Epoch 06 | Loss: 0.821 | Acc: 0.622\n",
      "Epoch 07 | Loss: 0.817 | Acc: 0.622\n",
      "Epoch 08 | Loss: 0.815 | Acc: 0.619\n",
      "Epoch 09 | Loss: 0.816 | Acc: 0.623\n",
      "Epoch 10 | Loss: 0.815 | Acc: 0.621\n",
      "Epoch 11 | Loss: 0.815 | Acc: 0.622\n",
      "Epoch 12 | Loss: 0.813 | Acc: 0.623\n",
      "Epoch 13 | Loss: 0.814 | Acc: 0.624\n",
      "Epoch 14 | Loss: 0.813 | Acc: 0.625\n",
      "Epoch 15 | Loss: 0.812 | Acc: 0.620\n",
      "Epoch 16 | Loss: 0.812 | Acc: 0.618\n",
      "Epoch 17 | Loss: 0.811 | Acc: 0.625\n",
      "Epoch 18 | Loss: 0.810 | Acc: 0.616\n",
      "Epoch 19 | Loss: 0.811 | Acc: 0.616\n",
      "Epoch 20 | Loss: 0.809 | Acc: 0.624\n",
      "Epoch 21 | Loss: 0.808 | Acc: 0.618\n",
      "Epoch 22 | Loss: 0.811 | Acc: 0.613\n",
      "Epoch 23 | Loss: 0.809 | Acc: 0.620\n",
      "Epoch 24 | Loss: 0.809 | Acc: 0.612\n",
      "Epoch 25 | Loss: 0.810 | Acc: 0.618\n",
      "Epoch 26 | Loss: 0.808 | Acc: 0.621\n",
      "Epoch 27 | Loss: 0.807 | Acc: 0.628\n",
      "Epoch 28 | Loss: 0.807 | Acc: 0.622\n",
      "Epoch 29 | Loss: 0.807 | Acc: 0.627\n",
      "Epoch 30 | Loss: 0.806 | Acc: 0.616\n",
      "Epoch 31 | Loss: 0.805 | Acc: 0.620\n",
      "Epoch 32 | Loss: 0.806 | Acc: 0.626\n",
      "Epoch 33 | Loss: 0.804 | Acc: 0.614\n",
      "Epoch 34 | Loss: 0.804 | Acc: 0.616\n",
      "Epoch 35 | Loss: 0.804 | Acc: 0.622\n",
      "Epoch 36 | Loss: 0.803 | Acc: 0.622\n",
      "Epoch 37 | Loss: 0.803 | Acc: 0.611\n",
      "Epoch 38 | Loss: 0.802 | Acc: 0.625\n",
      "Epoch 39 | Loss: 0.803 | Acc: 0.622\n",
      "Epoch 40 | Loss: 0.802 | Acc: 0.605\n",
      "Epoch 41 | Loss: 0.801 | Acc: 0.618\n",
      "Epoch 42 | Loss: 0.801 | Acc: 0.624\n",
      "Epoch 43 | Loss: 0.800 | Acc: 0.618\n",
      "Epoch 44 | Loss: 0.800 | Acc: 0.622\n",
      "Epoch 45 | Loss: 0.800 | Acc: 0.618\n",
      "Epoch 46 | Loss: 0.799 | Acc: 0.625\n",
      "Epoch 47 | Loss: 0.798 | Acc: 0.624\n",
      "Epoch 48 | Loss: 0.799 | Acc: 0.614\n",
      "Epoch 49 | Loss: 0.799 | Acc: 0.624\n",
      "Epoch 50 | Loss: 0.798 | Acc: 0.622\n",
      "\n",
      "Model Comparison:\n",
      "Model                |   Accuracy\n",
      "---------------------------------\n",
      "Original (1988)      |      0.630\n",
      "Improved (Positional) |      0.628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ---------------------\n",
    "# 1. Data Preparation\n",
    "# ---------------------\n",
    "def parse_data(filename):\n",
    "    \"\"\"Parse Qian & Sejnowski format files\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    sequences = []\n",
    "    current_seq = []\n",
    "    for line in content.split('\\n'):\n",
    "        if line.strip() == '<end>':\n",
    "            if current_seq:\n",
    "                sequences.append(current_seq)\n",
    "                current_seq = []\n",
    "        elif line and not line.startswith('#') and not line.startswith('<'):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                aa = parts[0]\n",
    "                ss = parts[1] if len(parts[1]) > 0 else '_'\n",
    "                current_seq.append((aa, ss))\n",
    "    return sequences\n",
    "\n",
    "# Load data - replace with actual file paths\n",
    "train_sequences = parse_data('protein-secondary-structure.train.txt')\n",
    "test_sequences = parse_data('protein-secondary-structure.test.txt')\n",
    "\n",
    "# ---------------------\n",
    "# 2. Encoding Utilities\n",
    "# ---------------------\n",
    "AMINO_ACIDS = ['A','C','D','E','F','G','H','I','K','L','M',\n",
    "               'N','P','Q','R','S','T','V','W','Y','X']  # X as spacer\n",
    "SS_CLASSES = ['h', 'e', '_']  # helix, sheet, coil\n",
    "\n",
    "def encode_window(window):\n",
    "    \"\"\"Qian & Sejnowski's local encoding scheme\"\"\"\n",
    "    encoded = []\n",
    "    for aa, _ in window:\n",
    "        vec = [0]*len(AMINO_ACIDS)\n",
    "        try:\n",
    "            idx = AMINO_ACIDS.index(aa)\n",
    "        except ValueError:\n",
    "            idx = -1  # handle unknown as spacer\n",
    "        vec[idx] = 1\n",
    "        encoded.extend(vec)\n",
    "    return encoded\n",
    "\n",
    "# ---------------------\n",
    "# 3. Original 1988 Network\n",
    "# ---------------------\n",
    "class QSNet(nn.Module):\n",
    "    \"\"\"Exact replica of Qian & Sejnowski (1988) architecture\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(13*21, 40)  # 13 residues Ã— 21 amino acids\n",
    "        self.fc2 = nn.Linear(40, 3)      # 3 output classes\n",
    "        self.sigmoid = nn.Sigmoid()       # Original activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------\n",
    "# 4. Dataset Preparation\n",
    "# ---------------------\n",
    "def create_dataset(sequences, window_size=13):\n",
    "    \"\"\"Create sliding window dataset with padding\"\"\"\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        # Add padding (6 spacers on each side)\n",
    "        padded = [('X', '_')]*6 + seq + [('X', '_')]*6\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # Get 13-residue window centered on current amino acid\n",
    "            window = padded[i:i+window_size]\n",
    "            encoded = encode_window(window)\n",
    "            label = SS_CLASSES.index(window[6][1])  # Center residue label\n",
    "            X.append(encoded)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y)\n",
    "\n",
    "# Create datasets\n",
    "X_train, y_train = create_dataset(train_sequences)\n",
    "X_test, y_test = create_dataset(test_sequences)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train, dtype=torch.long))  # Add dtype=torch.long\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test, dtype=torch.long))     # Add dtype=torch.long\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "# 5. Training Setup\n",
    "# ---------------------\n",
    "def train_model(model, train_loader, test_loader, epochs=50, model_name='Base'):\n",
    "    \"\"\"Training function with metrics tracking\"\"\"\n",
    "    train_losses = []\n",
    "    test_accs = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                all_preds.extend(preds.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = epoch_loss/len(train_loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        train_losses.append(train_loss)\n",
    "        test_accs.append(acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:02d} | Loss: {train_loss:.3f} | Acc: {acc:.3f}')\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_accs, label='Test Accuracy')\n",
    "    plt.title(f'{model_name} Learning Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_name}_curves.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=SS_CLASSES, yticklabels=SS_CLASSES)\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'{model_name}_cm.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return max(test_accs)\n",
    "\n",
    "# ---------------------\n",
    "# 6. Original Model Training\n",
    "# ---------------------\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize and train original model\n",
    "original_model = QSNet()\n",
    "orig_acc = train_model(original_model, train_loader, test_loader, \n",
    "                      epochs=50, model_name='Original')\n",
    "\n",
    "# ---------------------\n",
    "# 7. Improved Model with Positional Encoding\n",
    "# ---------------------\n",
    "class ImprovedQSNet(nn.Module):\n",
    "    \"\"\"Enhanced version with positional information\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(13*21 + 1, 40)  # Original + position\n",
    "        self.fc2 = nn.Linear(40, 3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_improved_dataset(sequences):\n",
    "    \"\"\"Dataset with positional encoding\"\"\"\n",
    "    X, y = [], []\n",
    "    for seq in sequences:\n",
    "        padded = [('X', '_')]*6 + seq + [('X', '_')]*6\n",
    "        seq_len = len(seq)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # Original encoding\n",
    "            window = padded[i:i+13]\n",
    "            encoded = encode_window(window)\n",
    "            \n",
    "            # Add positional feature (normalized 0-1)\n",
    "            position = i / seq_len\n",
    "            encoded.append(position)\n",
    "            \n",
    "            label = SS_CLASSES.index(window[6][1])\n",
    "            X.append(encoded)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y)\n",
    "\n",
    "# Create improved datasets\n",
    "X_train_imp, y_train_imp = create_improved_dataset(train_sequences)\n",
    "X_test_imp, y_test_imp = create_improved_dataset(test_sequences)\n",
    "\n",
    "train_dataset_imp = TensorDataset(torch.tensor(X_train_imp), torch.tensor(y_train_imp, dtype=torch.long))\n",
    "test_dataset_imp = TensorDataset(torch.tensor(X_test_imp), torch.tensor(y_test_imp, dtype=torch.long))\n",
    "\n",
    "train_loader_imp = DataLoader(train_dataset_imp, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_imp = DataLoader(test_dataset_imp, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Train improved model\n",
    "improved_model = ImprovedQSNet()\n",
    "imp_acc = train_model(improved_model, train_loader_imp, test_loader_imp,\n",
    "                     epochs=50, model_name='Improved')\n",
    "\n",
    "# ---------------------\n",
    "# 8. Results Comparison\n",
    "# ---------------------\n",
    "# Generate comparison table\n",
    "results = [\n",
    "    {'Model': 'Original (1988)', 'Accuracy': orig_acc},\n",
    "    {'Model': 'Improved (Positional)', 'Accuracy': imp_acc}\n",
    "]\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"{'Model':<20} | {'Accuracy':>10}\")\n",
    "print(\"-\"*33)\n",
    "for res in results:\n",
    "    print(f\"{res['Model']:<20} | {res['Accuracy']:>10.3f}\")\n",
    "\n",
    "# Generate comparison plot\n",
    "plt.figure()\n",
    "models = [res['Model'] for res in results]\n",
    "accs = [res['Accuracy'] for res in results]\n",
    "plt.bar(models, accs)\n",
    "plt.ylim(0.5, 0.7)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
